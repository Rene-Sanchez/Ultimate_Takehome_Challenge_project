{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime,timedelta\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with open('data_challenge.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% 36.62 - 18310 total users active <------------\n",
      "BEOFRE <-------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 12 columns):\n",
      "avg_dist                  50000 non-null float64\n",
      "avg_rating_by_driver      49799 non-null float64\n",
      "avg_rating_of_driver      41878 non-null float64\n",
      "avg_surge                 50000 non-null float64\n",
      "city                      50000 non-null object\n",
      "last_trip_date            50000 non-null object\n",
      "phone                     49604 non-null object\n",
      "signup_date               50000 non-null object\n",
      "surge_pct                 50000 non-null float64\n",
      "trips_in_first_30_days    50000 non-null int64\n",
      "ultimate_black_user       50000 non-null bool\n",
      "weekday_pct               50000 non-null float64\n",
      "dtypes: bool(1), float64(6), int64(1), object(4)\n",
      "memory usage: 4.2+ MB\n",
      "AFTER <-------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41445 entries, 0 to 49998\n",
      "Data columns (total 12 columns):\n",
      "avg_dist                  41445 non-null float64\n",
      "avg_rating_by_driver      41445 non-null float64\n",
      "avg_rating_of_driver      41445 non-null float64\n",
      "avg_surge                 41445 non-null float64\n",
      "city                      41445 non-null object\n",
      "last_trip_date            41445 non-null object\n",
      "phone                     41445 non-null object\n",
      "signup_date               41445 non-null object\n",
      "surge_pct                 41445 non-null float64\n",
      "trips_in_first_30_days    41445 non-null int64\n",
      "ultimate_black_user       41445 non-null bool\n",
      "weekday_pct               41445 non-null float64\n",
      "dtypes: bool(1), float64(6), int64(1), object(4)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# read in data and define which users are Active in the entire dataset using a list\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "frame = timedelta(days=30)\n",
    "start = datetime.strptime(data.last_trip_date.max(),'%Y-%m-%d') - frame\n",
    "active = []\n",
    "for i in data.last_trip_date:\n",
    "    inst = datetime.strptime(i,'%Y-%m-%d')\n",
    "    if inst > start:\n",
    "        active.append(1)\n",
    "    else:\n",
    "        active.append(0)\n",
    "        \n",
    "print('%',((np.sum(active) / len(active))*100),'-',np.sum(active),'total users active <------------')\n",
    " \n",
    "#seperate cases with missing values in individual dataframes\n",
    "print(\"BEOFRE <-------------\")\n",
    "data.info()\n",
    "\n",
    "df,missing_rating_of= (data.drop(data[data.avg_rating_of_driver.isnull()].index),\n",
    "                       data[data.avg_rating_of_driver.isnull()])\n",
    "\n",
    "df,missing_rating_by= (df.drop(df[df.avg_rating_by_driver.isnull()].index),\n",
    "                       df[df.avg_rating_by_driver.isnull()])\n",
    "\n",
    "df,missing_phone= (df.drop(df[df.phone.isnull()].index),\n",
    "                       df[df.phone.isnull()])\n",
    "print(\"AFTER <-------------\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observing the data above, we can see that most of the missing data is contained in the rating features. Given this, I will split these cases off to inspect them further to determine the impact of missing ratings on user retention. Additionally, with these cases removed I will construct a predictive model to predict whether or not a user will be active into the future and determine the importance of each feature based on the coefficient that is calculated in the model.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate labels list to determine which users were active in the past 30 days for the model data\n",
    "frame = timedelta(days=30)\n",
    "start = datetime.strptime(df.last_trip_date.max(),'%Y-%m-%d') - frame\n",
    "act = []\n",
    "for i in df.last_trip_date:\n",
    "    inst = datetime.strptime(i,'%Y-%m-%d')\n",
    "    if inst > start:\n",
    "        act.append(1)\n",
    "    else:\n",
    "        act.append(0)\n",
    "\n",
    "#dichotomize Phone Type and membership features\n",
    "df['phone'] = df['phone'].apply(lambda x: 1 if x=='Android' else 0)\n",
    "df['ultimate_black_user'] = df['ultimate_black_user'].apply(\n",
    "        lambda x: 1 if x==True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target class split: % 40.07238508867174 active\n"
     ]
    }
   ],
   "source": [
    "#Create dummy variables for city feature and create fonal model data to be using for prediction\n",
    "city_dummy = pd.get_dummies(df['city'],drop_first=True)\n",
    "\n",
    "#Remove columns that won't be used in the model, add Active label list, and generate final model Data.\n",
    "model_data = df.drop(['last_trip_date','signup_date','city'],axis=1)\n",
    "model_data = pd.concat([model_data,city_dummy],axis=1)\n",
    "model_data[\"Active\"] = act\n",
    "print(\"Target class split: %\",(model_data.Active.sum() / len(model_data))*100,'active')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing_rating_of_driver Percent inactive:% 80.69441024378233\n",
      "missing_rating_by_driver Percent inactive:% 89.55223880597015\n",
      "missing_ratings_of_both Percent inactive:% 70.1492537313433\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment=None \n",
    "#generate labels list to determine which users were active in the past 30 days for the missing values\n",
    "#dataframes and then print the proportion of users that are inactive for each dataframe\n",
    "\n",
    "of_act = []\n",
    "for i in missing_rating_of.last_trip_date:\n",
    "    inst = datetime.strptime(i,'%Y-%m-%d')\n",
    "    if inst > start:\n",
    "        of_act.append(1)\n",
    "    else:\n",
    "        of_act.append(0)\n",
    "missing_rating_of['Active'] = of_act\n",
    "\n",
    "\n",
    "by_act = []\n",
    "for i in missing_rating_by.last_trip_date:\n",
    "    inst = datetime.strptime(i,'%Y-%m-%d')\n",
    "    if inst > start:\n",
    "        by_act.append(1)\n",
    "    else:\n",
    "        by_act.append(0)\n",
    "missing_rating_by['Active'] = by_act\n",
    "\n",
    "#Cases that are missing both rating by driver and rating of driver\n",
    "missing_ratings = missing_rating_of[missing_rating_of['avg_rating_by_driver'].isnull() ==True]\n",
    "\n",
    "miss_act = []\n",
    "for i in missing_ratings.last_trip_date:\n",
    "    inst = datetime.strptime(i,'%Y-%m-%d')\n",
    "    if inst > start:\n",
    "        miss_act.append(1)\n",
    "    else:\n",
    "        miss_act.append(0)\n",
    "missing_ratings['Active'] = miss_act\n",
    "\n",
    "print('missing_rating_of_driver Percent inactive:%',\n",
    "      (1-(missing_rating_of.Active.sum() / len(missing_rating_of)))*100)\n",
    "print('missing_rating_by_driver Percent inactive:%',\n",
    "      (1-(missing_rating_by.Active.sum() / len(missing_rating_by)))*100)\n",
    "print('missing_ratings_of_both Percent inactive:%',\n",
    "      (1-(missing_ratings.Active.sum() / len(missing_ratings)))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We see here that cases that are missing ratings are very likely to be inactive with the rates of inactivity of this user being about double that of those in the model data used above. This indicates that cases where ratings are not given, especially by the driver, we can expect that the user is at significant risk of becoming inactive. This may be due to a poor experience during the ride so we could target theses users for additional surveys and incentives.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_process(a,b):\n",
    "#split data into training and test set and scales data\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(a, b, test_size = 0.25,random_state=42)\n",
    "    scaler_X = StandardScaler()\n",
    "    X_train = scaler_X.fit_transform(X_train)\n",
    "    X_test = scaler_X.transform(X_test)    \n",
    "    return X_train,X_test,Y_train,Y_test\n",
    "\n",
    "xtr,xte,ytr,yte = split_process(model_data.iloc[:,:-1],model_data['Active'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 160} Highest accuracy: 0.787214876299\n"
     ]
    }
   ],
   "source": [
    "#Tune Gradient Boosted Machine using grid search\n",
    "graboost = GradientBoostingClassifier(max_features='sqrt')\n",
    "parametergra = {'n_estimators':[155,165,160],'learning_rate':[.3,.2],\n",
    "              'max_depth':[3,4]}\n",
    "grid1 = GridSearchCV(estimator=graboost,param_grid=parametergra,\n",
    "                     scoring='accuracy',cv=5)\n",
    "grid1.fit(xtr,ytr)\n",
    "best_para_gra = grid1.best_params_\n",
    "best_acc_gra = grid1.best_score_\n",
    "\n",
    "print('Best parameters:',best_para_gra,'Highest accuracy:',best_acc_gra)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A Gradient Boosted Machine was decided on after trying both a Random Forest (RF) and Support Vector Machine (SVM) for its quick training time and high accuracy. A SVM took much too long to train and did not yield accuracy as high and a RF simply did not perform as well. This model incorporates many of the strengths of a RF and handles sparse data well like a SVM.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.85      0.83      6186\n",
      "          1       0.76      0.70      0.73      4176\n",
      "\n",
      "avg / total       0.79      0.79      0.79     10362\n",
      "\n",
      "[[5240  946]\n",
      " [1258 2918]]\n",
      "Gradient Boosted Tress Accuracy = 0.787299749083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_dist</th>\n",
       "      <th>avg_rating_by_driver</th>\n",
       "      <th>avg_rating_of_driver</th>\n",
       "      <th>avg_surge</th>\n",
       "      <th>phone</th>\n",
       "      <th>surge_pct</th>\n",
       "      <th>trips_in_first_30_days</th>\n",
       "      <th>ultimate_black_user</th>\n",
       "      <th>weekday_pct</th>\n",
       "      <th>King's Landing</th>\n",
       "      <th>Winterfell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBM</th>\n",
       "      <td>0.17728</td>\n",
       "      <td>0.09167</td>\n",
       "      <td>0.081108</td>\n",
       "      <td>0.097075</td>\n",
       "      <td>0.028614</td>\n",
       "      <td>0.116448</td>\n",
       "      <td>0.119356</td>\n",
       "      <td>0.026273</td>\n",
       "      <td>0.189843</td>\n",
       "      <td>0.04213</td>\n",
       "      <td>0.030203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     avg_dist  avg_rating_by_driver  avg_rating_of_driver  avg_surge  \\\n",
       "GBM   0.17728               0.09167              0.081108   0.097075   \n",
       "\n",
       "        phone  surge_pct  trips_in_first_30_days  ultimate_black_user  \\\n",
       "GBM  0.028614   0.116448                0.119356             0.026273   \n",
       "\n",
       "     weekday_pct  King's Landing  Winterfell  \n",
       "GBM     0.189843         0.04213    0.030203  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#employ tuned GBM model\n",
    "gbm = GradientBoostingClassifier(max_features='sqrt',\n",
    "                                 n_estimators=160,learning_rate=.2,max_depth= 4)\n",
    "gbm.fit(xtr,ytr)\n",
    "\n",
    "ypredgra = gbm.predict(xte)\n",
    "print(classification_report(yte,ypredgra))\n",
    "boost_result =confusion_matrix(yte,ypredgra)\n",
    "print(boost_result)\n",
    "boo_acc = (boost_result[1][1]+boost_result[0][0])/boost_result.sum()\n",
    "print(\"Gradient Boosted Tress Accuracy =\",boo_acc)\n",
    "\n",
    "feature_coef = pd.DataFrame(gbm.feature_importances_).transpose()\n",
    "feature_coef.columns = list(model_data.columns[:-1])\n",
    "feature_coef.index = ['GBM'] \n",
    "feature_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We can see above that the model performed well on the training set with ~%79 accuracy, precision, recall, and F1 score indicating it is well rounded in its consistency of predictions of both positive and negative cases. There is always the concern of over fitting a model and that concern is present here, however given the test result I am confident that give new data the model would still perform well.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 4.5, 'class_weight': '', 'solver': 'sag', 'tol': 0.8} Highest accuracy: 0.71144998874\n"
     ]
    }
   ],
   "source": [
    "#Tune Logisitc Regression using grid search\n",
    "logreg = LogisticRegression()\n",
    "parametervec = {'C':[4.3,4.5,4.6],'solver':['sag','liblinear'],'tol':[.75,.9,.8],\n",
    "                'class_weight':['balanced','']}\n",
    "grid2 = GridSearchCV(estimator=logreg,param_grid=parametervec,\n",
    "                     scoring='accuracy',cv=5)\n",
    "grid2.fit(xtr,ytr)\n",
    "best_para_vec = grid2.best_params_\n",
    "best_acc_vec = grid2.best_score_\n",
    "\n",
    "print('Best parameters:',best_para_vec,'Highest accuracy:',best_acc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I employed a Logistic Regression model to act as a baseline accuracy and provide additional insight into the contribution of each feature to the probability of one target class or the other. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.82      0.77      6186\n",
      "          1       0.67      0.56      0.61      4176\n",
      "\n",
      "avg / total       0.71      0.71      0.71     10362\n",
      "\n",
      "[[5057 1129]\n",
      " [1841 2335]]\n",
      "Logistic Regression Accuracy = 0.713375796178\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_dist</th>\n",
       "      <th>avg_rating_by_driver</th>\n",
       "      <th>avg_rating_of_driver</th>\n",
       "      <th>avg_surge</th>\n",
       "      <th>phone</th>\n",
       "      <th>surge_pct</th>\n",
       "      <th>trips_in_first_30_days</th>\n",
       "      <th>ultimate_black_user</th>\n",
       "      <th>weekday_pct</th>\n",
       "      <th>King's Landing</th>\n",
       "      <th>Winterfell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBM</th>\n",
       "      <td>0.182478</td>\n",
       "      <td>0.087590</td>\n",
       "      <td>0.075669</td>\n",
       "      <td>0.105198</td>\n",
       "      <td>0.029028</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>0.122585</td>\n",
       "      <td>0.029465</td>\n",
       "      <td>0.183744</td>\n",
       "      <td>0.047525</td>\n",
       "      <td>0.027729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Reg</th>\n",
       "      <td>-0.191431</td>\n",
       "      <td>-0.056726</td>\n",
       "      <td>-0.043851</td>\n",
       "      <td>-0.015060</td>\n",
       "      <td>-0.491365</td>\n",
       "      <td>0.128610</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.514373</td>\n",
       "      <td>0.032320</td>\n",
       "      <td>0.776729</td>\n",
       "      <td>0.326240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              avg_dist  avg_rating_by_driver  avg_rating_of_driver  avg_surge  \\\n",
       "GBM           0.182478              0.087590              0.075669   0.105198   \n",
       "Logistic Reg -0.191431             -0.056726             -0.043851  -0.015060   \n",
       "\n",
       "                 phone  surge_pct  trips_in_first_30_days  \\\n",
       "GBM           0.029028   0.108989                0.122585   \n",
       "Logistic Reg -0.491365   0.128610                0.431373   \n",
       "\n",
       "              ultimate_black_user  weekday_pct  King's Landing  Winterfell  \n",
       "GBM                      0.029465     0.183744        0.047525    0.027729  \n",
       "Logistic Reg             0.514373     0.032320        0.776729    0.326240  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#employ tuned Logistic regression model\n",
    "reg = LogisticRegression(C=4.3,tol=.9,solver='sag')\n",
    "reg.fit(xtr,ytr)\n",
    "\n",
    "ypredreg = reg.predict(xte)\n",
    "print(classification_report(yte,ypredreg))\n",
    "reg_result =confusion_matrix(yte,ypredreg)\n",
    "print(reg_result)\n",
    "reg_acc = (reg_result[1][1]+reg_result[0][0])/reg_result.sum()\n",
    "print(\"Logistic Regression Accuracy =\",reg_acc)\n",
    "feature_coef1 = pd.DataFrame(reg.coef_,index=['Logistic Reg'])\n",
    "feature_coef1.columns = list(model_data.columns[:-1])\n",
    "feature_coef= pd.concat([feature_coef,feature_coef1])\n",
    "feature_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Above we have the feature importance as determined by the Gradient Boosted Machine (GBM) and the coefficients produced by the Regression model that indicate the contribution of a given feature. The coefficients are helpful in determining the nature of the relationship between the features and the Active statuses.\n",
    "\n",
    "According the GBM results, Percent of Weekday Trips, Average Distance, and Trips in the First 30 Days appear to be the most important features according to the GBM model. The Logistic Regression suggests Average Distance is negatively related to Active status, indicating that as average distance goes down the user is more likely to become inactive.\n",
    "\n",
    "Both Trips in First 30 Days and Percent of Weekday Trips are positively related with Active status which suggests that users with a higher Percent of Weekday Trips and more Trips in the First 30 Days are more likely to be Active, and vice versa.\n",
    "\n",
    "Given this insight this insight, Ultimate should look to target new users with promotions to incentivize users to use their service during the weekdays. Additionally, Ultimate should experiment with a cost structure that incentivizes users to take longer trips to promote consistent use and engagement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
